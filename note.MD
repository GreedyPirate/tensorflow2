### L1范数
向量𝒙的所有元素绝对值之和

### L2范数
向量𝒙的所有元素的平方和，再开根号

### ∞ −范数
向量𝒙的所有元素绝对值的最大值

## dense层参数
x: [1000, 784]
1000张图片，28*28=784个特征

w: [784, 256]
784个特征的权重(kernel, wight), 本层的输出

b:[256]
输出层的bais


强化学习中希望最大化回报函数，则可按着梯度方向更新
𝜃′ = 𝜃 + 𝜂 ∙ ∇𝜃ℒ

# 过拟合

## 验证集
为了挑选 模型超参数和检测过拟合现象，一般需要将原来的训练集再次切分为新的训练集和验证集(Validation set)
即数据集需要切分为训练集、验证集和测试集3个子集,常见的划分60%-20%-20%

我们需要在模型训练时能够挑选出较合适的模型超参数，判断模型是否过拟
合等，因此需要将训练集再次切分为训练集和验证集，划分过的 训练集与原来的训练集的功能一致
用于训练模型的参数，而验证集则用于选择模型的超参数(模型选择，Model selection)，它的功能包括：
1.根据验证集的性能表现来调整学习率、权值衰减系数、训练次数等
2.根据验证集的性能表现来重新调整网络拓扑结构
3.根据验证集的性能表现判断是否过拟合和欠拟合。

### 验证集与测试集的区别
验证集与测试集的区别在于，算法设计人员可以根据验证集的表现来调整模型的各种
超参数的设置，提升模型的泛化能力，但是测试集的表现却不能用来反馈模型的调整，否
则测试集将和验证集的功能重合，因此在测试集上的性能表现将无法代表模型的泛化能
力

为了防止用测试集来调参，可以选择生成多个测试集，这样即使开发人员使用了其中一个测试集来挑选
模型，我们还可以使用其它测试集来评价模型，这也是 Kaggle 竞赛常用的做法

## 过拟合和欠拟合
当观测到过拟合现象时，可以从新设计网络模型的容量，如降低网络的层数、降低网
络的参数量、添加正则化手段、添加假设空间的约束等，使得模型的实际容量降低，从而
减轻或解决过拟合现象；当观测到欠拟合现象时，可以尝试增大网络的容量，如加深网络
的层数、增加网络的参数量，尝试更复杂的网络结构。

# 如何解决过拟合
实际情况中，有可能随着训练 Epoch 数的增加，出现过拟合，解决方式是观察准确率，选择合适的 Epoch 就提前停止训练

## 正则化
也是防止过拟合的

### L0,L1,L2
L0正则不可导
L1正则常用，参数绝对值之和
L2正则常用, 参数平方和
正则化函数添加到损失函数上
```python
import tensorflow as tf

# 创建网络参数 w1,w2
w1 = tf.random.normal([4,3])
w2 = tf.random.normal([4,2])
# 计算 L1 正则化项
loss_reg = tf.reduce_sum(tf.math.abs(w1))\
+ tf.reduce_sum(tf.math.abs(w2))

# 创建网络参数 w1,w2
w1 = tf.random.normal([4,3])
w2 = tf.random.normal([4,2])
# 计算 L2 正则化项
loss_reg = tf.reduce_sum(tf.square(w1))\
+ tf.reduce_sum(tf.square(w2))
```

## Dropout

Dropout通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；
但是在测试时，Dropout会恢复所有的连接，保证模型测试时获得最好的性能

当前节点与前一层的所有输入节点相连，每条连接是否断开符合某种预设的概率分布，如断开概率为𝑝的伯努利分布

```python
import tensorflow as tf

# 添加 dropout 操作，断开概率为 0.5
x = tf.nn.dropout(x, rate=0.5)
# 或添加 Dropout 层，断开概率为 0.5
model.add(layers.Dropout(rate=0.5))
```

## 数据增强
对图片来说，就是把一张图片，通过旋转，缩放，裁剪等多种不改变样本label的手段，扩大到n张
然后变相增大了数据集，测试集越多，理论上过拟合概率越小